{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "from gulpio2 import GulpDirectory\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Any, Dict, List, Sequence, Union\n",
    "\n",
    "from systems import EpicActionRecognitionSystem\n",
    "from systems import EpicActionRecogintionDataModule\n",
    "\n",
    "from utils.metrics import compute_metrics\n",
    "from utils.actions import action_id_from_verb_noun\n",
    "from scipy.special import softmax\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from frame_sampling import RandomSampler\n",
    "from torchvideo.samplers import FrameSampler\n",
    "from torchvideo.samplers import frame_idx_to_list\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pkl_path: Path, frame_sampler: FrameSampler, features_dim: int = 256):\n",
    "        self.pkl_path = pkl_path\n",
    "        self.frame_sampler = frame_sampler\n",
    "        self.features_dim = features_dim\n",
    "        self.pkl_dict = Dict[str, Any]\n",
    "        self.frame_cumsum = np.array([0.])\n",
    "        self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        with open(self.pkl_path, 'rb') as f:\n",
    "            self.pkl_dict = pickle.load(f)\n",
    "            frame_counts = [label['num_frames'] for label in self.pkl_dict['labels']]\n",
    "            self.frame_cumsum = np.cumsum(np.concatenate([self.frame_cumsum, frame_counts]), dtype=int)\n",
    "    \n",
    "    def _video_from_narration_id(self, key: int):\n",
    "#         video_no = self.pkl_dict['narration_id'].index(narration_id)\n",
    "        l = self.frame_cumsum[key]\n",
    "        r = self.frame_cumsum[key+1]\n",
    "        return self.pkl_dict['features'][l:r]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_dict['narration_id'])\n",
    "    \n",
    "    def __getitem__(self, key: int):\n",
    "        features = self._video_from_narration_id(key)\n",
    "        video_length = features.shape[0]\n",
    "        \n",
    "        assert video_length == self.pkl_dict['labels'][key]['num_frames']\n",
    "        if video_length < frame_sampler.frame_count:\n",
    "            raise ValueError(f\"Video too short to sample {n_frames} from\")\n",
    "        \n",
    "        sample_idxs = np.array(frame_idx_to_list(frame_sampler.sample(video_length)))\n",
    "        return (features[sample_idxs], self.pkl_dict['labels'][key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 8\n",
    "frame_sampler = RandomSampler(frame_count=n_frames, snippet_length=1, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PickleDataset('../datasets/epic/features/p01_01_1_features.pkl', frame_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xd, xs = dataset._video_from_narration_id(dataset.pkl_dict['narration_id'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.pkl_dict['labels']\n",
    "# for inp, lables in :\n",
    "#     print(inp, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, frame_count: int):\n",
    "        super().__init__()\n",
    "        self.frame_count = frame_count\n",
    "        self.fc1 = nn.Linear(256 * frame_count, 512)\n",
    "        self.fc2 = nn.Linear(512, 397)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256 * self.frame_count)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(frame_count=8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        dynamic_ncols=True\n",
    "    ):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        labels = t.tensor([[labels['verb_class'],labels['noun_class']]], device = device)\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        out = net(inputs.to(device))\n",
    "#         scores = {\n",
    "#             'verb': out[:,:97].cpu().numpy(),\n",
    "#             'noun': out[:,97:].cpu().numpy(),\n",
    "#             'narration_id': labels['narration_id']\n",
    "#         }\n",
    "    \n",
    "#         verb_top_n = scores['verb'][0].argsort()[::-1][0]\n",
    "#         noun_top_n = scores['noun'][0].argsort()[::-1][0]\n",
    "        \n",
    "#         output = np.concatenate([verb_top_n,noun_top_n])\n",
    "\n",
    "        out_v = out[:,:97]\n",
    "        out_n = out[:,97:]\n",
    "        \n",
    "        loss_v = criterion(out_v, labels[:,0])\n",
    "#         loss_n = criterion(out_n, labels[:,1])\n",
    "        \n",
    "        loss_v.backward()\n",
    "#         loss_n.backward()\n",
    "\n",
    "        \n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss_v.item()#+loss_n.item())/2\n",
    "        if i % 2 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
