{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7dba9b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'V_MTRN' from 'models.esvs' (/media/deepthought/SCRATCH/Tom/eve_home/C1-Action-Recognition-TSN-TRN-TSM/src/models/esvs.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-b3f324169ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mV_MTRN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiPickleDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'V_MTRN' from 'models.esvs' (/media/deepthought/SCRATCH/Tom/eve_home/C1-Action-Recognition-TSN-TRN-TSM/src/models/esvs.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models.esvs import V_MTRN\n",
    "from datasets.pickle_dataset import MultiPickleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from frame_sampling import RandomSampler\n",
    "from torchvideo.samplers import frame_idx_to_list\n",
    "from attribution.online_shapley_value_attributor import OnlineShapleyAttributor\n",
    "import pandas as pd\n",
    "\n",
    "from subset_samplers import ConstructiveRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cd6aad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "dtype = torch.float\n",
    "\n",
    "n_frames = 4\n",
    "\n",
    "def no_collate(args):\n",
    "    return args\n",
    "\n",
    "frame_sampler = RandomSampler(frame_count=n_frames, snippet_length=1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0b1f2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [_MTRN(frame_count=i) for i in range(1,9)]\n",
    "for j, m in enumerate(models):\n",
    "    models[j].load_state_dict(torch.load(f'../datasets/epic/models/mtrn-frames={j+1}.pt'))\n",
    "\n",
    "model = models[n_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f026e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiPickleDataset('../datasets/epic/features/p01_features.pkl')\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326f2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.softmax(torch.rand((1,97)), dim=-1)\n",
    "data = iter(dataloader)\n",
    "\n",
    "inp, lab = data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85785bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frames(video):\n",
    "    video_length = len(video)\n",
    "    if video_length < n_frames:\n",
    "        raise ValueError(f\"Video too short to sample {n_frames} from\")\n",
    "    sample_idxs = np.array(frame_idx_to_list(frame_sampler.sample(video_length)))\n",
    "    return sample_idxs, video[sample_idxs]\n",
    "\n",
    "\n",
    "# input_ = torch.cat(inp).to(dtype=dtype)\n",
    "\n",
    "# input_.shape\n",
    "# subsample_frames(inp.squeeze())\n",
    "sample_idx, sample_video = subsample_frames(inp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c43449e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1280]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-e8fec9d81ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/deepthought/SCRATCH/Tom/miniconda3/envs/epic/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/deepthought/SCRATCH/Tom/eve_home/C1-Action-Recognition-TSN-TRN-TSM/src/models/esvs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_verb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1280]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "out = models(sample_video.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ee00608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 397])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = torch.softmax(out.cpu(), dim=-1)\n",
    "\n",
    "nouns = torch.softmax(torch.rand((1,300)), dim=-1)\n",
    "\n",
    "\n",
    "result_scores = torch.cat((verbs, nouns), dim=-1)\n",
    "result_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2079a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors = pd.read_csv('../datasets/epic/labels/verb_class_priors.csv', index_col='verb_class')['prior'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e8f08d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributor = OnlineShapleyAttributor(\n",
    "    single_scale_models=models,\n",
    "    priors=class_priors,\n",
    "    n_classes=len(class_priors),\n",
    "    device=device,\n",
    "    subset_sampler=ConstructiveRandomSampler(max_samples=128, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2fea6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "esvs, _ = attributor.explain(sample_video.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2c5dccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_esvs = torch.softmax(torch.rand((1,n_frames,300)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "25b3b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_esvs = esvs.cpu().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "38069f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_esvs = torch.cat((verb_esvs, noun_esvs), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "695cebc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 397])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_esvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9de88850",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-542e4d0f11d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "scores.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "25f085c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7a9a2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab['narration_id'] = lab['narration_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6447c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P01_01_0'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab['narration_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409bb68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
