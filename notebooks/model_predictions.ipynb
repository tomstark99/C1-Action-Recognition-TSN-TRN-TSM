{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa6c24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models.esvs import _MTRN\n",
    "from datasets.pickle_dataset import MultiPickleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from frame_sampling import RandomSampler\n",
    "from torchvideo.samplers import frame_idx_to_list\n",
    "from attribution.online_shapley_value_attributor import OnlineShapleyAttributor\n",
    "import pandas as pd\n",
    "\n",
    "from subset_samplers import ConstructiveRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4f0f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "dtype = torch.float\n",
    "\n",
    "n_frames = 4\n",
    "\n",
    "def no_collate(args):\n",
    "    return args\n",
    "\n",
    "frame_sampler = RandomSampler(frame_count=n_frames, snippet_length=1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0771c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [_MTRN(frame_count=i) for i in range(1,9)]\n",
    "for j, m in enumerate(models):\n",
    "    models[j].load_state_dict(torch.load(f'../datasets/epic/models/mtrn-frames={j+1}.pt'))\n",
    "\n",
    "model = models[n_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b458689",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiPickleDataset('../datasets/epic/features/p01_features.pkl')\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57542ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.softmax(torch.rand((1,97)), dim=-1)\n",
    "data = iter(dataloader)\n",
    "\n",
    "inp, lab = data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a503728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frames(video):\n",
    "    video_length = len(video)\n",
    "    if video_length < n_frames:\n",
    "        raise ValueError(f\"Video too short to sample {n_frames} from\")\n",
    "    sample_idxs = np.array(frame_idx_to_list(frame_sampler.sample(video_length)))\n",
    "    return sample_idxs, video[sample_idxs]\n",
    "\n",
    "\n",
    "# input_ = torch.cat(inp).to(dtype=dtype)\n",
    "\n",
    "# input_.shape\n",
    "# subsample_frames(inp.squeeze())\n",
    "sample_idx, sample_video = subsample_frames(inp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff723771",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(sample_video.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09d8fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 397])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = torch.softmax(out.cpu(), dim=-1)\n",
    "\n",
    "nouns = torch.softmax(torch.rand((1,300)), dim=-1)\n",
    "\n",
    "\n",
    "result_scores = torch.cat((verbs, nouns), dim=-1)\n",
    "result_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a979bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors = pd.read_csv('../datasets/epic/labels/verb_class_priors.csv', index_col='verb_class')['prior'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a7722a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributor = OnlineShapleyAttributor(\n",
    "    single_scale_models=models,\n",
    "    priors=class_priors,\n",
    "    n_classes=len(class_priors),\n",
    "    device=device,\n",
    "    subset_sampler=ConstructiveRandomSampler(max_samples=128, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ad6d29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "esvs, _ = attributor.explain(sample_video.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b3a67b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_esvs = torch.softmax(torch.rand((1,n_frames,300)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a84bc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_esvs = esvs.cpu().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "04058fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_esvs = torch.cat((verb_esvs, noun_esvs), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "15ad72e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 397])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_esvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "41b27387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2395e-07, 3.3908e-07, 5.8338e-10, 1.0000e+00, 3.4592e-07, 5.1184e-09,\n",
       "          2.7350e-08, 8.3875e-19, 1.9700e-10, 9.0772e-14, 5.9277e-14, 1.1191e-09,\n",
       "          5.7197e-22, 6.9220e-10, 1.5641e-10, 6.7009e-14, 4.1518e-19, 9.9002e-11,\n",
       "          1.8142e-18, 1.1444e-30, 2.1583e-19, 3.3834e-13, 6.4151e-19, 4.7155e-15,\n",
       "          1.5246e-11, 1.6208e-30, 9.3500e-13, 2.4585e-22, 1.0455e-12, 4.0832e-13,\n",
       "          7.2530e-29, 4.7336e-27, 3.0561e-22, 1.4567e-14, 1.6909e-13, 1.7889e-18,\n",
       "          1.9405e-27, 1.8237e-32, 1.4351e-29, 6.0800e-15, 1.8333e-13, 1.2956e-12,\n",
       "          1.6992e-19, 1.0244e-13, 2.1771e-22, 2.0248e-26, 1.2540e-13, 5.9344e-26,\n",
       "          2.7389e-12, 3.4341e-18, 8.5247e-13, 4.5339e-19, 3.4088e-21, 9.9019e-24,\n",
       "          9.7781e-14, 1.2768e-13, 1.3115e-29, 1.1716e-13, 2.8644e-13, 3.1215e-29,\n",
       "          5.6449e-13, 3.9466e-13, 1.8200e-13, 1.5892e-20, 3.9636e-21, 3.9318e-12,\n",
       "          1.4236e-12, 4.3685e-13, 2.3514e-13, 4.7388e-13, 2.0958e-13, 1.2054e-16,\n",
       "          9.7227e-25, 1.2760e-10, 3.9869e-13, 3.7658e-13, 4.7793e-13, 4.9296e-16,\n",
       "          3.4731e-13, 5.2565e-13, 1.8324e-12, 2.9904e-13, 7.3674e-36, 3.9245e-13,\n",
       "          1.7145e-13, 5.0043e-13, 3.1004e-13, 1.3427e-25, 1.1271e-12, 1.5590e-13,\n",
       "          1.5253e-13, 1.5519e-13, 8.7150e-13, 2.3989e-13, 4.2422e-13, 4.6293e-13,\n",
       "          7.9636e-14]]),\n",
       " tensor([[9.4560e-09, 5.8881e-09, 2.2754e-11, 1.0000e+00, 2.0398e-08, 7.3016e-13,\n",
       "          1.3210e-09, 9.6652e-23, 4.9202e-13, 4.3871e-14, 5.1728e-16, 9.6374e-11,\n",
       "          1.4313e-25, 9.0958e-10, 1.3656e-10, 3.9199e-15, 6.2137e-18, 9.2693e-12,\n",
       "          6.1538e-18, 7.9200e-37, 9.3409e-25, 5.6946e-15, 3.0402e-18, 4.6625e-18,\n",
       "          1.5371e-12, 1.4578e-36, 1.1866e-12, 8.7216e-21, 9.3765e-15, 9.1286e-13,\n",
       "          7.3156e-32, 5.1774e-27, 4.9297e-27, 6.0422e-18, 4.2739e-14, 2.5841e-21,\n",
       "          4.5996e-32, 8.4132e-29, 1.7851e-33, 1.7504e-18, 8.6371e-13, 9.7686e-12,\n",
       "          3.2410e-23, 6.1984e-16, 1.6970e-28, 2.0990e-33, 1.7502e-14, 1.2962e-28,\n",
       "          2.5471e-15, 5.6552e-22, 3.4353e-13, 8.6457e-22, 9.4417e-19, 3.9653e-23,\n",
       "          2.6358e-13, 3.3220e-14, 3.7190e-30, 6.1732e-13, 1.0145e-14, 2.3285e-29,\n",
       "          8.5067e-15, 1.0742e-14, 2.2704e-14, 9.7695e-22, 3.5875e-19, 2.7726e-12,\n",
       "          1.8069e-11, 2.4000e-14, 1.7699e-12, 1.4796e-13, 2.6915e-14, 2.2698e-17,\n",
       "          8.6589e-28, 2.1454e-11, 1.7822e-14, 1.5362e-13, 1.0167e-14, 1.2013e-16,\n",
       "          1.6188e-14, 1.6358e-12, 1.3171e-13, 2.0485e-14, 2.4102e-43, 2.2834e-13,\n",
       "          6.9559e-14, 2.2211e-12, 1.5663e-13, 3.4287e-30, 5.8890e-13, 4.0681e-13,\n",
       "          3.1090e-13, 9.4475e-14, 6.3736e-13, 3.0097e-13, 5.8999e-13, 1.1312e-13,\n",
       "          1.9988e-14]]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.cpu().unsqueeze(0), verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d4370b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "36c27ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab['narration_id'] = lab['narration_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4889838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P01_01_0'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab['narration_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56ce9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
