{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gulpio2 import GulpDirectory\n",
    "from pathlib import Path\n",
    "from moviepy.editor import ImageSequenceClip, clips_array\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Any, Dict, List, Sequence, Union\n",
    "\n",
    "from systems import EpicActionRecognitionSystem\n",
    "\n",
    "from utils.metrics import compute_metrics\n",
    "from utils.actions import action_id_from_verb_noun\n",
    "from scipy.special import softmax\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP TORCH VARIABLES\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "dtype = t.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    # LOAD IN SAVED CHECKPOINT\n",
    "    ckpt = t.load('../models/trn_rgb.ckpt', map_location='cpu')\n",
    "\n",
    "    # CREATE CONFIG FROM CHECKPOINT\n",
    "    cfg = OmegaConf.create(ckpt['hyper_parameters'])\n",
    "    OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "    # SET GULP DIRECTORY\n",
    "    cfg.data._root_gulp_dir = '/home/ts/C1-Action-Recognition-TSN-TRN-TSM/datasets/epic/gulp/rgb'\n",
    "\n",
    "    # CREATE MODEL\n",
    "    model = EpicActionRecognitionSystem(cfg)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    return model.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleFeatureWriter(FeatureWriter):\n",
    "    \n",
    "    def __init__(self, pkl_path: Path, features_dim: int):\n",
    "        self.pkl_path = pkl_path\n",
    "        self.narration_ids = []\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        self.\n",
    "        \n",
    "    def:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, backbone_2d: nn.Module, device: t.device, dtype: t.float, frame_batch_size: int = 128):\n",
    "        self.model = backbone_2d\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.frame_batch_size = frame_batch_size\n",
    "    \n",
    "    def extract(self, dataset: GulpDirectory, feature_writer: Dict[str, Any]) -> int:\n",
    "        total_instances = 0\n",
    "        self.model.eval()\n",
    "        for i, c in enumerate(tqdm(dataset)):\n",
    "            if i == 0:\n",
    "                for batch_input, batch_labels in c:\n",
    "        \n",
    "                    batch_input = np.array(batch_input).transpose(0,3,1,2)\n",
    "                    batch_input = t.tensor(batch_input, device=self.device, dtype=self.dtype)\n",
    "                    batch_input = batch_input.unsqueeze(0)\n",
    "\n",
    "                    batch_size, n_frames = batch_input.shape[:2]\n",
    "                    flattened_batch_input = batch_input.view((-1, *batch_input.shape[2:]))\n",
    "\n",
    "                    n_chunks = int(np.ceil(len(flattened_batch_input)/128))\n",
    "                    chunks = t.chunk(flattened_batch_input, n_chunks, dim=0)\n",
    "                    flatten_batch_features = []\n",
    "                    for chunk in chunks:\n",
    "                        chunk = chunk.unsqueeze(0)\n",
    "                        with t.no_grad():\n",
    "                            chunk_features = self.model.features(chunk.to(self.device))\n",
    "                            chunk_features = self.model.new_fc(chunk_features)\n",
    "                            flatten_batch_features.append(chunk_features.squeeze(0))\n",
    "                    flatten_batch_features = t.cat(flatten_batch_features, dim=0)\n",
    "                    batch_features = flatten_batch_features.view((batch_size, \n",
    "                                                                  n_frames, \n",
    "                                                                  *flatten_batch_features.shape[1:]))\n",
    "\n",
    "                    total_instances += batch_size\n",
    "                    self._append(batch_features, batch_labels, batch_size, feature_writer)\n",
    "        return total_instances\n",
    "\n",
    "    def _append(self, batch_features, batch_labels, batch_size, feature_writer):\n",
    "        self.end_frame += batch_labels['num_frames']\n",
    "        feature_writer['features'].extend(batch_features.cpu().numpy())\n",
    "        feature_writer['metadata'].extend(np.array((batch_labels['narration_id'],\n",
    "                                                    batch_labels['narration'],\n",
    "                                                    batch_labels['verb'],\n",
    "                                                    batch_labels['verb_class'],\n",
    "                                                    batch_labels['noun'],\n",
    "                                                    batch_labels['noun_class'],\n",
    "                                                    self.start_frame,\n",
    "                                                    self.end_frame)))\n",
    "        self.start_frame = self.end_frame\n",
    "        print(batch_labels['narration_id'], \"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01_01_0 DONE\n",
      "P01_01_1 DONE\n",
      "P01_01_10 DONE\n",
      "P01_01_100 DONE\n",
      "P01_01_101 DONE\n",
      "P01_01_102 DONE\n",
      "P01_01_103 DONE\n",
      "P01_01_104 DONE\n",
      "P01_01_105 DONE\n",
      "P01_01_106 DONE\n",
      "P01_01_107 DONE\n",
      "P01_01_108 DONE\n",
      "P01_01_109 DONE\n",
      "P01_01_11 DONE\n",
      "P01_01_110 DONE\n",
      "P01_01_111 DONE\n",
      "P01_01_112 DONE\n",
      "P01_01_113 DONE\n",
      "P01_01_114 DONE\n",
      "P01_01_115 DONE\n",
      "P01_01_116 DONE\n",
      "P01_01_117 DONE\n",
      "P01_01_118 DONE\n",
      "P01_01_12 DONE\n",
      "P01_01_120 DONE\n",
      "P01_01_121 DONE\n",
      "P01_01_122 DONE\n",
      "P01_01_123 DONE\n",
      "P01_01_124 DONE\n",
      "P01_01_125 DONE\n",
      "P01_01_126 DONE\n",
      "P01_01_127 DONE\n",
      "P01_01_128 DONE\n",
      "P01_01_129 DONE\n",
      "P01_01_13 DONE\n",
      "P01_01_130 DONE\n",
      "P01_01_131 DONE\n",
      "P01_01_132 DONE\n",
      "P01_01_133 DONE\n",
      "P01_01_134 DONE\n",
      "P01_01_135 DONE\n",
      "P01_01_136 DONE\n",
      "P01_01_137 DONE\n",
      "P01_01_138 DONE\n",
      "P01_01_139 DONE\n",
      "P01_01_14 DONE\n",
      "P01_01_140 DONE\n",
      "P01_01_141 DONE\n",
      "P01_01_142 DONE\n",
      "P01_01_143 DONE\n",
      "P01_01_144 DONE\n",
      "P01_01_145 DONE\n",
      "P01_01_146 DONE\n",
      "P01_01_147 DONE\n",
      "P01_01_148 DONE\n",
      "P01_01_149 DONE\n",
      "P01_01_15 DONE\n",
      "P01_01_150 DONE\n",
      "P01_01_151 DONE\n",
      "P01_01_152 DONE\n",
      "P01_01_153 DONE\n",
      "P01_01_154 DONE\n",
      "P01_01_155 DONE\n",
      "P01_01_156 DONE\n",
      "P01_01_157 DONE\n",
      "P01_01_158 DONE\n",
      "P01_01_159 DONE\n",
      "P01_01_16 DONE\n",
      "P01_01_160 DONE\n",
      "P01_01_161 DONE\n",
      "P01_01_162 DONE\n",
      "P01_01_163 DONE\n",
      "P01_01_164 DONE\n",
      "P01_01_165 DONE\n",
      "P01_01_166 DONE\n",
      "P01_01_167 DONE\n",
      "P01_01_168 DONE\n",
      "P01_01_169 DONE\n",
      "P01_01_17 DONE\n",
      "P01_01_170 DONE\n",
      "P01_01_171 DONE\n",
      "P01_01_172 DONE\n",
      "P01_01_173 DONE\n",
      "P01_01_174 DONE\n",
      "P01_01_175 DONE\n",
      "P01_01_176 DONE\n",
      "P01_01_177 DONE\n",
      "P01_01_178 DONE\n",
      "P01_01_179 DONE\n",
      "P01_01_18 DONE\n",
      "P01_01_180 DONE\n",
      "P01_01_181 DONE\n",
      "P01_01_182 DONE\n",
      "P01_01_183 DONE\n",
      "P01_01_184 DONE\n",
      "P01_01_185 DONE\n",
      "P01_01_186 DONE\n",
      "P01_01_187 DONE\n",
      "P01_01_188 DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [02:09, 32.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01_01_189 DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_train = GulpDirectory('../datasets/epic/gulp/rgb/rgb_test/')\n",
    "\n",
    "features_and_meta = {\n",
    "   'features': [],\n",
    "   'metadata': []\n",
    "}\n",
    "# for i, c in enumerate(rgb_train):\n",
    "#     if i == 0:\n",
    "#         for frames, meta in tqdm(c):\n",
    "#             xd = frames.shape\n",
    "extractor = FeatureExtractor(generate_model(), device, dtype)\n",
    "extractor.extract(rgb_train, features_and_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('p01_01_chunk_1_features.pkl', 'wb') as f:\n",
    "  pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         7 function calls in 0.003 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.003    0.003 <__array_function__ internals>:2(concatenate)\n",
      "        1    0.000    0.000    0.003    0.003 <ipython-input-13-f1de2ff8162e>:5(numpy_concat)\n",
      "        1    0.000    0.000    0.003    0.003 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 multiarray.py:143(concatenate)\n",
      "        1    0.000    0.000    0.003    0.003 {built-in method builtins.exec}\n",
      "        1    0.003    0.003    0.003    0.003 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features1 = features_and_meta['features']\n",
    "\n",
    "import cProfile\n",
    "\n",
    "def numpy_concat(x):\n",
    "    np.concatenate(x)\n",
    "\n",
    "cProfile.run('numpy_concat(features1)')\n",
    "\n",
    "# allow = np.concatenate(features_and_meta['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'195'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features0 = features_and_meta['metadata']\n",
    "\n",
    "f_1 = features0[0:8]\n",
    "\n",
    "start = f_1[7]\n",
    "start\n",
    "# rgb_train['P01_01_1'][1], \n",
    "allow[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
