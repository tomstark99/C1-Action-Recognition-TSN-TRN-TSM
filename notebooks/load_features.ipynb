{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "from gulpio2 import GulpDirectory\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Any, Dict, List, Sequence, Union, Tuple\n",
    "\n",
    "from systems import EpicActionRecognitionSystem\n",
    "from systems import EpicActionRecogintionDataModule\n",
    "\n",
    "from utils.metrics import compute_metrics\n",
    "from utils.actions import action_id_from_verb_noun\n",
    "from scipy.special import softmax\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from frame_sampling import RandomSampler\n",
    "from torchvideo.samplers import FrameSampler\n",
    "from torchvideo.samplers import frame_idx_to_list\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pkl_path: Path, frame_sampler: FrameSampler, features_dim: int = 256):\n",
    "        self.pkl_path = pkl_path\n",
    "        self.frame_sampler = frame_sampler\n",
    "        self.features_dim = features_dim\n",
    "        self.pkl_dict = Dict[str, Any]\n",
    "        self.frame_cumsum = np.array([0.])\n",
    "        self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        with open(self.pkl_path, 'rb') as f:\n",
    "            self.pkl_dict = pickle.load(f)\n",
    "            frame_counts = [label['num_frames'] for label in self.pkl_dict['labels']]\n",
    "            self.frame_cumsum = np.cumsum(np.concatenate([self.frame_cumsum, frame_counts]), dtype=int)\n",
    "    \n",
    "    def _video_from_narration_id(self, key: int):\n",
    "        l = self.frame_cumsum[key]\n",
    "        r = self.frame_cumsum[key+1]\n",
    "        return self.pkl_dict['features'][l:r]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_dict['narration_id'])\n",
    "    \n",
    "    def __getitem__(self, key: int):\n",
    "        features = self._video_from_narration_id(key)\n",
    "        video_length = features.shape[0]\n",
    "        \n",
    "        assert video_length == self.pkl_dict['labels'][key]['num_frames']\n",
    "        if video_length < self.frame_sampler.frame_count:\n",
    "            raise ValueError(f\"Video too short to sample {self.frame_sampler.frame_count} from\")\n",
    "        \n",
    "        sample_idxs = np.array(frame_idx_to_list(frame_sampler.sample(video_length)))\n",
    "        return (features[sample_idxs], { k: self.pkl_dict['labels'][key][k] for k in ['narration_id','verb_class','noun_class'] })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, frame_count: int):\n",
    "        super().__init__()\n",
    "        self.frame_count = frame_count\n",
    "        self.fc1 = nn.Linear(256 * frame_count, 512)\n",
    "        self.fc2 = nn.Linear(512, 397)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256 * self.frame_count)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 8\n",
    "frame_sampler = RandomSampler(frame_count=n_frames, snippet_length=1, test=False)\n",
    "\n",
    "# def collate(data):\n",
    "#     inputs, labels = zip(*data)\n",
    "    \n",
    "#     inp = t.tensor(inputs)\n",
    "#     print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PickleDataset('../datasets/epic/features/p01_features.pkl', frame_sampler)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self, \n",
    "        model: nn.Module, \n",
    "        dataloader: DataLoader, \n",
    "        optimiser: optim.Adadelta, \n",
    "        device: t.device, \n",
    "        log_interval: int = 100\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.optimiser = optimiser\n",
    "        self.device = device\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "    def _step(self, batch: Tuple[t.Tensor, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "\n",
    "        data, labels = batch\n",
    "        self.optimiser.zero_grad()\n",
    "        outputs = self.model(data.to(self.device))\n",
    "        tasks = {\n",
    "            'verb': {\n",
    "                'output': outputs[:,:97],\n",
    "                'preds': outputs[:,:97].argmax(-1),\n",
    "                'labels': labels['verb_class'],\n",
    "                'weight': 1\n",
    "            },\n",
    "            'noun': {\n",
    "                'output': outputs[:,97:],\n",
    "                'preds': outputs[:,97:].argmax(-1),\n",
    "                'labels': labels['noun_class'],\n",
    "                'weight': 1\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        step_results = dict()\n",
    "        loss = 0.0\n",
    "        n_tasks = len(tasks)\n",
    "        for task, d in tasks.items():\n",
    "            task_loss = F.cross_entropy(d['output'], d['labels'].to(device))\n",
    "            loss += d['weight'] * task_loss\n",
    "            \n",
    "        step_results['narration_id'] = labels['narration_id']\n",
    "        step_results['loss'] = loss / n_tasks\n",
    "        return step_results\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, data in enumerate(self.dataloader):\n",
    "            \n",
    "            step_results = self._step(data)\n",
    "            loss = step_results['loss']\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, batch_idx + 1, running_loss / self.log_interval))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.237\n",
      "[1,   101] loss: 9.541\n",
      "[1,   201] loss: 6.007\n",
      "[1,   301] loss: 5.143\n",
      "[1,   401] loss: 4.564\n",
      "[1,   501] loss: 4.461\n",
      "[1,   601] loss: 4.341\n",
      "[1,   701] loss: 4.411\n",
      "[1,   801] loss: 4.211\n",
      "[1,   901] loss: 3.993\n",
      "[1,  1001] loss: 4.157\n",
      "[1,  1101] loss: 4.143\n",
      "[1,  1201] loss: 4.061\n",
      "[1,  1301] loss: 3.679\n",
      "[1,  1401] loss: 4.289\n",
      "[1,  1501] loss: 4.056\n",
      "[1,  1601] loss: 3.700\n",
      "[1,  1701] loss: 3.787\n",
      "[1,  1801] loss: 4.019\n",
      "[1,  1901] loss: 3.939\n",
      "[1,  2001] loss: 3.848\n",
      "[1,  2101] loss: 3.979\n",
      "[1,  2201] loss: 3.974\n",
      "[1,  2301] loss: 3.945\n",
      "[1,  2401] loss: 3.755\n",
      "[1,  2501] loss: 3.820\n",
      "[1,  2601] loss: 3.653\n",
      "[1,  2701] loss: 3.717\n",
      "[1,  2801] loss: 3.973\n",
      "[1,  2901] loss: 3.579\n",
      "[1,  3001] loss: 3.755\n",
      "[1,  3101] loss: 3.654\n",
      "[1,  3201] loss: 4.076\n",
      "[1,  3301] loss: 3.512\n",
      "[1,  3401] loss: 3.813\n",
      "[1,  3501] loss: 3.577\n",
      "[1,  3601] loss: 3.646\n",
      "[1,  3701] loss: 3.630\n",
      "[1,  3801] loss: 3.513\n",
      "[1,  3901] loss: 3.806\n",
      "[1,  4001] loss: 3.655\n",
      "[1,  4101] loss: 3.631\n",
      "[1,  4201] loss: 3.511\n",
      "[1,  4301] loss: 3.618\n",
      "[1,  4401] loss: 4.031\n",
      "[1,  4501] loss: 3.713\n",
      "[1,  4601] loss: 3.687\n",
      "[1,  4701] loss: 3.730\n",
      "[1,  4801] loss: 3.442\n",
      "[1,  4901] loss: 3.609\n",
      "[1,  5001] loss: 3.568\n",
      "[1,  5101] loss: 3.253\n",
      "[1,  5201] loss: 3.594\n",
      "[1,  5301] loss: 3.488\n",
      "[1,  5401] loss: 3.702\n",
      "[1,  5501] loss: 3.711\n",
      "[2,     1] loss: 0.022\n",
      "[2,   101] loss: 3.521\n",
      "[2,   201] loss: 3.591\n",
      "[2,   301] loss: 3.493\n",
      "[2,   401] loss: 3.366\n",
      "[2,   501] loss: 3.592\n",
      "[2,   601] loss: 3.443\n",
      "[2,   701] loss: 3.438\n",
      "[2,   801] loss: 3.503\n",
      "[2,   901] loss: 3.629\n",
      "[2,  1001] loss: 3.958\n",
      "[2,  1101] loss: 3.393\n",
      "[2,  1201] loss: 3.275\n",
      "[2,  1301] loss: 3.444\n",
      "[2,  1401] loss: 3.470\n",
      "[2,  1501] loss: 3.609\n",
      "[2,  1601] loss: 3.463\n",
      "[2,  1701] loss: 3.749\n",
      "[2,  1801] loss: 3.444\n",
      "[2,  1901] loss: 3.290\n",
      "[2,  2001] loss: 3.378\n",
      "[2,  2101] loss: 3.392\n",
      "[2,  2201] loss: 3.358\n",
      "[2,  2301] loss: 3.379\n",
      "[2,  2401] loss: 3.504\n",
      "[2,  2501] loss: 3.541\n",
      "[2,  2601] loss: 3.680\n",
      "[2,  2701] loss: 3.354\n",
      "[2,  2801] loss: 3.699\n",
      "[2,  2901] loss: 3.558\n",
      "[2,  3001] loss: 3.447\n",
      "[2,  3101] loss: 3.531\n",
      "[2,  3201] loss: 3.354\n",
      "[2,  3301] loss: 3.390\n",
      "[2,  3401] loss: 3.496\n",
      "[2,  3501] loss: 3.414\n",
      "[2,  3601] loss: 3.572\n",
      "[2,  3701] loss: 3.534\n",
      "[2,  3801] loss: 3.530\n",
      "[2,  3901] loss: 3.332\n",
      "[2,  4001] loss: 3.243\n",
      "[2,  4101] loss: 3.563\n",
      "[2,  4201] loss: 3.461\n",
      "[2,  4301] loss: 3.582\n",
      "[2,  4401] loss: 3.305\n",
      "[2,  4501] loss: 3.454\n",
      "[2,  4601] loss: 3.470\n",
      "[2,  4701] loss: 3.309\n",
      "[2,  4801] loss: 3.256\n",
      "[2,  4901] loss: 3.388\n",
      "[2,  5001] loss: 3.402\n",
      "[2,  5101] loss: 3.470\n",
      "[2,  5201] loss: 3.532\n",
      "[2,  5301] loss: 3.304\n",
      "[2,  5401] loss: 3.552\n",
      "[2,  5501] loss: 3.437\n"
     ]
    }
   ],
   "source": [
    "model = Net(frame_count=8).to(device)\n",
    "optimiser = optim.Adadelta(model.parameters(), lr=0.005)\n",
    "classifier = Classifier(model, dataloader, optimiser, device, log_interval=100)\n",
    "\n",
    "for epoch in range(2):\n",
    "    classifier.train(epoch)\n",
    "    \n",
    "# dataiter = iter(dataloader)\n",
    "# inputs, labels = dataiter.next()\n",
    "\n",
    "# out = model(inputs.to(device))\n",
    "\n",
    "# nouns = out[:,:97]\n",
    "\n",
    "# nouns.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
